{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the ROOT-LBN Tutorial\n",
    "\n",
    "* Created: 16/06/2020\n",
    "* Contact: dennis . noll [at] cern . ch\n",
    "\n",
    "This notbook showcases, how a simple classification between two physics processes can be implemented using the Lorentz Boost Network ([LBN](https://arxiv.org/abs/1812.09722)).\n",
    "We will start with two **root files** and end with a fully trained classifier which can be applied on **numpy arrays**.\n",
    "\n",
    "For input data we use NANOAOD simulations following `RunIIFall17NanoAODv6`.\n",
    "The two root files contain physics events from two different processes with the same final state:\n",
    "* `signal.root`: GluGluToHHTo2B2WToLNu2J_node_SM\n",
    "* `background.root`: TTToSemiLeptonic\n",
    "\n",
    "All the needed software is pinned in the `environment.yml` file.\n",
    "It can be installed and sourced using [`conda`](https://docs.conda.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import uproot\n",
    "import uproot_methods\n",
    "import awkward\n",
    "from lbn import LBN, LBNLayer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tools.keras import Normal\n",
    "\n",
    "TLorentzVectorArray = uproot_methods.classes.TLorentzVector.TLorentzVectorArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for correct tensorflow version and GPU availability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU: []\n"
     ]
    }
   ],
   "source": [
    "assert tf.__version__.startswith(\"2\")\n",
    "print(\"Available GPU:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Define some functions that handle uproot data formats and convert `TLorentzVectors` consisting of the transverse momentum, the pseudorapidity, the phi angle and the mass to four vectors consisting of the energy and the three momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_lorentz(tree, name=\"Jet\"):\n",
    "    return TLorentzVectorArray.from_ptetaphim(\n",
    "        tree[\"%s_pt\" % name].array(),\n",
    "        tree[\"%s_eta\" % name].array(),\n",
    "        tree[\"%s_phi\" % name].array(),\n",
    "        tree[\"%s_mass\" % name].array()\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_to_array(tree, mask, name=\"Jet\", n=4):\n",
    "    lorentz = tree_to_lorentz(tree, name=name)\n",
    "    array = np.array([\n",
    "        lorentz[mask].E[:, :n].pad(n).fillna(0).flatten().reshape(-1, n),\n",
    "        lorentz[mask].x[:, :n].pad(n).fillna(0).flatten().reshape(-1, n),\n",
    "        lorentz[mask].y[:, :n].pad(n).fillna(0).flatten().reshape(-1, n),\n",
    "        lorentz[mask].z[:, :n].pad(n).fillna(0).flatten().reshape(-1, n),\n",
    "    ])\n",
    "    return np.moveaxis(array, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for signal and background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/eos/user/d/dnoll/HH/lbn/%s.root\"\n",
    "DATA_PATH = \"/net/scratch/dn801636/projects/root_to_lbn/data/%s.root\"\n",
    "signal, background = uproot.open(DATA_PATH % \"signal\"), uproot.open(DATA_PATH % \"background\")\n",
    "signal_tree, background_tree = signal[\"Events\"], background[\"Events\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example we will select events with exactly one electron with transverse momentum greater than 25 GeV and equal or more than four jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_selection(tree):\n",
    "    e_mask = tree[\"nElectron\"].array() == 1\n",
    "    pt_mask = (tree[\"Electron_pt\"].array() > 25).any()\n",
    "    jet_mask = tree[\"nJet\"].array() >=4\n",
    "    return e_mask * pt_mask * jet_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a boolean mask for the event selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_mask, background_mask = event_selection(signal_tree), event_selection(background_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of low and high level input data. As low level inputs, we will use the four momenta of the four jets with the highest momentum, and the electron, as high level feature we will use the variable \"MET_pt\" which is defined on the event level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_level(tree, mask):\n",
    "    jets = tree_to_array(tree, mask, name=\"Jet\", n=4)\n",
    "    electron = tree_to_array(tree, mask, name=\"Electron\", n=1)\n",
    "    events = np.concatenate([jets, electron], axis=1)\n",
    "    return events\n",
    "\n",
    "def get_high_level(tree, mask, variables=[\"MET_pt\"]):\n",
    "    output = np.array([tree[variable].array()[mask] for variable in variables])\n",
    "    return np.moveaxis(output, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get low and high level data, which will be used for the LBN and the following DNN respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ll, signal_hl = get_low_level(signal_tree, signal_mask), get_high_level(signal_tree, signal_mask)\n",
    "background_ll, background_hl = get_low_level(background_tree, background_mask), get_high_level(background_tree, background_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode targets to one_hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones(signal_ll.shape[0])[..., None] * np.array([1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_targets = np.ones(signal_ll.shape[0])[..., None] * np.array([0, 1])\n",
    "background_targets = np.ones(background_ll.shape[0])[..., None] * np.array([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a factor ~10 more background than signal events and have to explicitly care that the network does not only concentrate on the background class.\n",
    "One option is to use a class weight on each event in the loss function, giving a small weight to the background and a large weight to the signal.\n",
    "This approach works fine if the size differences between the classes are not too large.\n",
    "We will use it in this example. Because the differences are already pretty large (factor ~10), our validation accuracy will fluctuate.\n",
    "If the differences are to large, we have to crop the larger class or, as an even better approach, sample up the smaller class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_signal = signal_ll.shape[0]\n",
    "# background_ll, background_hl, background_targets = background_ll[:n_signal], background_hl[:n_signal], background_targets[:n_signal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate and shuffle signal and background samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(*arrays):\n",
    "    first_array = arrays[0]\n",
    "    assert [len(first_array) == len(array) for array in arrays[1:]]\n",
    "    p = np.random.permutation(len(first_array))\n",
    "    return [array[p] for array in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = np.concatenate([signal_ll, background_ll], axis=0).astype(np.float32)\n",
    "hl = np.concatenate([signal_hl, background_hl], axis=0).astype(np.float32)\n",
    "targets = np.concatenate([signal_targets, background_targets], axis=0)\n",
    "\n",
    "ll, hl, targets = shuffle_together(ll, hl, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape LL (146209, 5, 4)\n",
      "Shape HL (146209, 1)\n",
      "Shape targets (146209, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape LL\", ll.shape)\n",
    "print(\"Shape HL\", hl.shape)\n",
    "print(\"Shape targets\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data, use 90% for training and 10% for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into 131588 training and 14621 validation events\n"
     ]
    }
   ],
   "source": [
    "n_data = ll.shape[0]\n",
    "split = int(n_data * 0.9)\n",
    "print(\"Splitting data into\", split, \"training and\", n_data-split, \"validation events\")\n",
    "\n",
    "ll_train, hl_train, ll_val, hl_val = ll[:split], hl[:split], ll[split:], hl[split:]\n",
    "y_train, y_val = targets[:split], targets[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the class weights which we will later use in the fit procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weight: {0: 0.5550371647015055, 1: 5.042385156573321}\n"
     ]
    }
   ],
   "source": [
    "i_targets = np.argmax(targets, axis=1)\n",
    "class_weight = compute_class_weight('balanced', classes=np.unique(i_targets), y=i_targets)\n",
    "class_weight = dict(enumerate(class_weight))\n",
    "print(\"Class weight:\", class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "Now we set up the model with `tf.keras`.\n",
    "First some general network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "n_nodes = 256\n",
    "activation = \"relu\"\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functional API, where we can feed two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_inputs = tf.keras.Input(shape=(5, 4), name=\"LL\")\n",
    "hl_inputs = tf.keras.Input(shape=(1,), name=\"HL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Lorentz Boost Network and use it as first layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbn_layer = LBNLayer(ll_inputs.shape, 10, boost_mode=LBN.PAIRS, features=[\"E\", \"pt\", \"eta\", \"phi\", \"m\", \"pair_cos\"])\n",
    "lbn_features = lbn_layer(ll_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features from the LBN (energy, transverse momentum, ...) with a BatchNormalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_lbn_features = tf.keras.layers.BatchNormalization()(lbn_features)\n",
    "normalized_hl_vars = Normal(ref=hl, axis=0)(hl_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the normalized features from the LBN and the high level features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.concatenate([normalized_lbn_features, normalized_hl_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the feed forward network constisting of some dense layers with dropout between the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dense(n_nodes, activation=activation)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(dropout)(x)\n",
    "x = tf.keras.layers.Dense(n_nodes, activation=activation)(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(dropout)(x)\n",
    "outputs = tf.keras.layers.Dense(n_classes)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[ll_inputs, hl_inputs], outputs=outputs, name='lbn_dnn')\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary of the created model with the number of total trainable variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "At last, fit the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    [ll_train, hl_train],\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=1024,\n",
    "    class_weight=class_weight,\n",
    "    validation_data=([ll_val, hl_val], y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot a simple confusion matrix for the final classificaiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    figure = plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=\"summer\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(i, j, cm[i, j], horizontalalignment=\"center\", size=18)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"hh_tt.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = model.predict([ll_val, hl_val], batch_size=1024)\n",
    "cm = confusion_matrix(np.argmax(y_val, axis=1), np.argmax(pred_val, axis=1))\n",
    "plot_confusion_matrix(cm, [\"TT\", \"HH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cms",
   "language": "python",
   "name": "cms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
