{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the ROOT-LBN Tutorial\n",
    "\n",
    "* Created: 16/06/2020\n",
    "* Contact: dennis . noll [at] cern . ch\n",
    "\n",
    "This notbook showcases, how a simple classification between two physics processes can be implemented using the Lorentz Boost Network ([LBN](https://arxiv.org/abs/1812.09722)).\n",
    "We will start with two **root files** and end with a fully trained classifier which can be applied on **numpy arrays**.\n",
    "\n",
    "For input data we use NANOAOD simulations following `RunIIFall17NanoAODv6`.\n",
    "The two root files contain physics events from two different processes with the same final state:\n",
    "* `signal.root`: GluGluToHHTo2B2WToLNu2J_node_SM\n",
    "* `background.root`: TTToSemiLeptonic\n",
    "\n",
    "All the needed software is pinned in the `environment.yml` file.\n",
    "It can be installed and sourced using [`conda`](https://docs.conda.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import uproot\n",
    "import uproot_methods\n",
    "import awkward\n",
    "from lbn import LBN, LBNLayer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "TLorentzVectorArray = uproot_methods.classes.TLorentzVector.TLorentzVectorArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tf.__version__.startswith(\"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "First we will define some useful fuctions for data shuffling, converting root trees into numpy arrays, shaping the numpy arrays the way we want to use them, selecting our events and plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data shuffling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(*arrays):\n",
    "    first_array = arrays[0]\n",
    "    assert [len(first_array) == len(array) for array in arrays[1:]]\n",
    "    p = np.random.permutation(len(first_array))\n",
    "    return [array[p] for array in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some functions to handle uproot data formats and to convert `TLorentzVectors` consisting of the transverse momentum, the pseudorapidity, the phi angle and the mass to four vectors consisting of the energy and the three momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_lorentz(tree, name=\"Jet\"):\n",
    "    return TLorentzVectorArray.from_ptetaphim(\n",
    "        tree[\"%s_pt\" % name].array(),\n",
    "        tree[\"%s_eta\" % name].array(),\n",
    "        tree[\"%s_phi\" % name].array(),\n",
    "        tree[\"%s_mass\" % name].array()\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_to_array(tree, mask, name=\"Jet\", n=4):\n",
    "    lorentz = tree_to_lorentz(tree, name=name)\n",
    "    array = np.array([\n",
    "        lorentz[mask].E[:, :n],\n",
    "        lorentz[mask].x[:, :n],\n",
    "        lorentz[mask].y[:, :n],\n",
    "        lorentz[mask].z[:, :n],\n",
    "    ])\n",
    "    return np.moveaxis(array, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple example we will select events with exactly one electron with transverse momentum greater than 25 GeV and equal or more than four jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_selection(tree):\n",
    "    e_mask = tree[\"nElectron\"].array() == 1\n",
    "    pt_mask = any(tree[\"Electron_pt\"].array() > 25)\n",
    "    jet_mask = tree[\"nJet\"].array() >=4\n",
    "    return e_mask * pt_mask * jet_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of low and high level input data. As low level inputs, we will use the four momenta of the four jets with the highest momentum, and the electron, as high level feature we will use the variable \"MET_pt\" which is defined on the event level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_low_level(tree, mask):\n",
    "    jets = tree_to_array(tree, mask, name=\"Jet\")\n",
    "    electron = tree_to_array(tree, mask, name=\"Electron\", n=1)\n",
    "    events = np.concatenate([jets, electron], axis=1)\n",
    "    return events\n",
    "\n",
    "def get_high_level(tree, mask, variables=[\"MET_pt\"]):\n",
    "    output = np.array([tree[variable].array()[mask] for variable in variables])\n",
    "    return np.moveaxis(output, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a simple confusion matrix for the final classificaiton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names):\n",
    "    figure = plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    threshold = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"hh_tt.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Load data for signal and background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/eos/user/d/dnoll/HH/lbn/%s.root\"\n",
    "signal, background = uproot.open(DATA_PATH % \"signal\"), uproot.open(DATA_PATH % \"background\")\n",
    "signal_tree, background_tree = signal[\"Events\"], background[\"Events\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a boolean mask for the event selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_mask, background_mask = event_selection(signal_tree), event_selection(background_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get low and high level data, which will be used for the LBN and the following DNN respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_ll, signal_hl = get_low_level(signal_tree, signal_mask), get_high_level(signal_tree, signal_mask)\n",
    "background_ll, background_hl = get_low_level(background_tree, background_mask), get_high_level(background_tree, background_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode targets to one_hot vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_targets = tf.keras.backend.one_hot(np.ones(signal_ll.shape[0]), 2)\n",
    "background_targets = tf.keras.backend.one_hot(np.zeros(background_ll.shape[0]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a factor ~10 more background than signal events and have to explicitly care that the network does not only concentrate on the background class.\n",
    "One option is to use a class weight on each event in the loss function, giving a small weight to the background and a large weight to the signal.\n",
    "This approach works fine if the size differences between the classes are not too large.\n",
    "We will use it in this example. Because the differences are already pretty large (factor ~10), our validation accuracy will fluctuate.\n",
    "If the differences are to large, we have to crop the larger class or, as an even better approach, sample up the smaller class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_signal = signal_ll.shape[0]\n",
    "# background_ll, background_hl, background_targets = background_ll[:n_signal], background_hl[:n_signal], background_targets[:n_signal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate and shuffle signal and background samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = np.concatenate([signal_ll, background_ll], axis=0).astype(np.float32)\n",
    "hl = np.concatenate([signal_hl, background_hl], axis=0).astype(np.float32)\n",
    "targets = np.concatenate([signal_targets, background_targets], axis=0)\n",
    "\n",
    "ll, hl, targets = shuffle_together(ll, hl, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize high-level features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_mean, hl_std = np.mean(hl, axis=0), np.std(hl, axis=0)\n",
    "hl = (hl - hl_mean) / hl_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data, use 90% for training and 10% for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = ll.shape[0]\n",
    "split = int(n_data * 0.9)\n",
    "\n",
    "ll_train, hl_train, ll_val, hl_val = ll[:split], hl[:split], ll[split:], hl[split:]\n",
    "y_train, y_val = targets[:split], targets[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the class weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_targets = np.argmax(targets, axis=1)\n",
    "class_weight = compute_class_weight('balanced', np.unique(i_targets), i_targets)\n",
    "class_weight = dict(enumerate(class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "Now we set up the model with `tf.keras`.\n",
    "First some general network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "n_nodes = 256\n",
    "activation = \"relu\"\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functional API, where we can feed two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_inputs = tf.keras.Input(shape=(5, 4))\n",
    "hl_inputs = tf.keras.Input(shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Lorentz Boost Network and use it as first layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbn_layer = LBNLayer(ll_inputs.shape, 10, boost_mode=LBN.PAIRS, features=[\"E\", \"pt\", \"eta\", \"phi\", \"m\", \"pair_cos\"])\n",
    "lbn_features = lbn_layer(ll_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features from the LBN (energy, transverse momentum, ...) with a BatchNormalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_lbn_features = tf.keras.layers.BatchNormalization()(lbn_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the normalized features from the LBN and the high level features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.concatenate([normalized_lbn_features, hl_inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the feed forward network constisting of some dense layers with dropout between the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Dense(n_nodes, activation=activation)(x)\n",
    "x = tf.keras.layers.Dropout(dropout)(x)\n",
    "x = tf.keras.layers.Dense(n_nodes, activation=activation)(x)\n",
    "x = tf.keras.layers.Dropout(dropout)(x)\n",
    "outputs = tf.keras.layers.Dense(n_classes)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[ll_inputs, hl_inputs], outputs=outputs, name='lbn_dnn')\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a summary of the created model with the number of total trainable variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "At last, fit the model to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    [ll_train, hl_train],\n",
    "    y_train,\n",
    "    epochs=5,\n",
    "    batch_size=1024,\n",
    "    class_weight=class_weight,\n",
    "    validation_data=([ll_val, hl_val], y_val),\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix of the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = model.predict([ll_val, hl_val], batch_size=1024)\n",
    "confusion_matrix = confusion_matrix(np.argmax(y_val, axis=1), np.argmax(pred_val, axis=1))\n",
    "plot_confusion_matrix(confusion_matrix, [\"TT\", \"HH\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root_lbn",
   "language": "python",
   "name": "root_lbn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
